# -*- coding: utf-8 -*-
"""falcon7b instruct.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IuGDll937PH1DN9S50iJxrV8_CvAFUWV

# FALCON-7B Instruct
Falcon-7B-Instruct is a 7B parameters causal decoder-only model built by TII based on Falcon-7B and finetuned on a mixture of chat/instruct datasets. It is made available under the Apache 2.0 license.

### **Why use Falcon-7B-Instruct?**
*   You are looking for a ready-to-use chat/instruct model based on Falcon-7B.

*  Falcon-7B is a strong base model, outperforming comparable open-source models (e.g., MPT-7B, StableLM, RedPajama etc.), thanks to being trained on 1,500B tokens of RefinedWeb enhanced with curated corpora. See the OpenLLM Leaderboard.

*  It features an architecture optimized for inference, with FlashAttention (Dao et al., 2022) and multiquery (Shazeer et al., 2019).

*  It is made available under a permissive Apache 2.0 license allowing for commercial use, without any royalties or restrictions.

üí¨ This is an instruct model, which may not be ideal for further finetuning. If you are interested in building your own instruct/chat model, we recommend starting from Falcon-7B.
"""

# install dependencies
!pip install transformers
!pip install einops
!pip install accelerate

"""### Loading the "tiiuae/falcon-7b-instruct" Language Model with Hugging Face Transformers

This code snippet sets up the "tiiuae/falcon-7b-instruct" model for text generation tasks:


1.   Imports: We import AutoTokenizer and AutoModelForCausalLM from transformers, and torch for PyTorch functionalities.
2.   Model Loading: The model variable specifies the model name, and tokenizer is initialized for this model.
3.   Pipeline Setup: We create a text generation pipeline, configuring it for efficiency and automatic device selection.





"""

from transformers import AutoTokenizer, AutoModelForCausalLM
import transformers
import torch

# load model
model = "tiiuae/falcon-7b-instruct"
tokenizer = AutoTokenizer.from_pretrained(model)

falcon_pipeline = transformers.pipeline("text-generation",
                                        model=model,
                                        tokenizer=tokenizer,
                                        torch_dtype=torch.bfloat16,
                                        trust_remote_code=True,
                                        device_map="auto"
                                        )

def get_completion_falcon(input_text):
    # Shortened description and examples
    counselor_role = "You are a caring and empathetic mental health counsellor focused on providing positive and constructive advice."

    few_shot_examples = """
    Question: I'm scared of being alone.
    Answer: It's common to feel this way. Open communication with your partner can help deepen your connection.

    Question: I'm uncomfortable about coming out as bisexual to my parents.
    Answer: Coming out can be stressful, but being true to yourself is important. Educating your family might help them understand better.
    """

    # Construct the prompt with the new user input
    prompt = f"{counselor_role}\n{few_shot_examples}\nUser: {input_text}\nResponse from falcon-7b-instruct:"

    # Generate response from the model
    falcon_response = falcon_pipeline(prompt,
                                      max_length=500,
                                      do_sample=True,
                                      top_k=10,
                                      num_return_sequences=1,
                                      eos_token_id=tokenizer.eos_token_id)

    # Extract the text from the model's response
    response_text = falcon_response[0]['generated_text'] if falcon_response else "No response generated."
    return response_text

def test_get_completion_falcon():
    # Sample input text for testing
    sample_input = "I've been feeling overwhelmed and stressed lately because of work and personal issues. What can I do to cope better?"

    # Call the function with the sample input
    response = get_completion_falcon(sample_input)

    # Print the response
    print("Generated Response:\n", response)

# Call the test function
test_get_completion_falcon()

# def get_completion_falcon(input_text):

#     # Define the role of the mental health counselor
#     counselor_role = """
#     You are a caring and empathetic mental health counsellor. Your mission is to provide support and guidance to individuals seeking assistance with their emotional well-being. Emphasize positive and constructive advice, avoiding harmful suggestions.
#     As a counsellor, you are well-versed in addressing a variety of topics, including anxiety, depression, self-expression, interpersonal dynamics, and developmental challenges. Your role is to offer thoughtful and empathetic responses tailored to each individual's unique situation.
#     As a counsellor, it's crucial to prioritize ethical behaviour. Please refrain from providing harmful advice, engaging in unethical practices, or suggesting actions that may pose risks to the user's well-being.
#     """

#     # Define few-shot examples from your dataset
#     few_shot_examples = """
#     #### Example 1:
#     Question: I'm scared that I am with this man so I won't be alone. He should be with somebody who deserves him if this is the case, and I don‚Äôt want to hurt him.
#     Answer: While not wanting to be alone may not be the best reason to be in a relationship, it is probably more common and normal a reason than you think. ¬†Since you seem to care about your friend ("don't want to hurt him"), I imagine there are many other reasons that you are together. ¬†I suggest that you talk about this open-heartedly with each other. ¬†The idea of being afraid of being alone sounds like an honest starting place. ¬†Don't try to "figure out" whether you should be with him. ¬†Just talk. ¬†The communication is likely to shine light on deepening connection for BOTH OF YOU.In the meantime, your idea that you don't deserve him is rooted in a "core lie" that you are telling yourself. ¬†You can read about "core lies" and much more in my book, Living Yes, a Handbook for Being Human. ¬†Check out www.LivingYes.org.Be easy on yourself. ¬†You are deserving!~Mark


#     #### Example 2:
#     Question: My parents seem okay with other sexualities, but normally they only talk about being gay. When they do talk about bisexuality, they say things like ‚Äúthey'll do anything‚Äù or things that make me very uncomfortable because I am bisexual. I don't know if I am ready to come out to them.
# Answer: Coming out to family members can cause a lot of anxiety. However, ¬†although I cannot promise what their reaction will be, the benefit is that you will no longer have to hide who you are. Perhaps researching or getting information on bisexuality can help your family understand what it truly is to be bisexual. Many times, ¬†people are just not educated on certain things.
#     """

#     # Construct the prompt with the new user input
#     prompt = f"{counselor_role}\n{few_shot_examples}\n#### User: \n{input_text}\n\n#### Response from falcon-7b-instruct:"

#     # Generate response from the model
#     falcon_response = falcon_pipeline(prompt,
#                                       max_length=500,
#                                       do_sample=True,
#                                       top_k=10,
#                                       num_return_sequences=1,
#                                       eos_token_id=tokenizer.eos_token_id)

#     # Extract the text from the model's response
#     response_text = falcon_response[0]['generated_text'] if falcon_response else "No response generated."

#     return response_text

# sample_input = "I've been feeling overwhelmed and stressed lately because of work and personal issues. What can I do to cope better?"
# response = get_completion_falcon(sample_input)
# print(response)

